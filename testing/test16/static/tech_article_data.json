[
    {
        "article_name": "system-design",
        "aticle_data": [
            
            {"title": "System Design : a component overview", "image_src": "..\\static\\images\\tech\\system-design.png", "article_para": "As I embarked on the journey of mastering system design to excel in technical job interviews, I found myself immersed in the rich landscape of distributed systems. Combining resources from platforms like educative.io and literature such as System Design Interview An Insiderâ€™s Guide by Alex Xu, I began unraveling the intricacies of various components crucial to modern system architecture. The parallels I drew between theoretical knowledge and real-world systems were enlightening, offering a profound understanding that traditional university education had not provided.", "markdown_data": ""},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Load Balancer:** Vital for managing server requests,load balancers distribute incoming traffic among servers to prevent overloads. By monitoring server statuses and workload, they intelligently route requests to the least busy servers."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Databases:** Serving as repositories for data, databases come in relational or non-relational forms, chosen based on specific use cases and accessibility needs. Replication algorithms ensure fault tolerance and data integrity."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Content Delivery Network (CDN):** CDN accelerates content delivery by caching data closer to end-users, reducing latency. This geographically distributed network ensures efficient content delivery across diverse locations."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Sequencer:** Responsible for assigning unique identifiers to data, sequencers utilize solutions like UUIDs or database-backed schemes to ensure data integrity and prevent conflicts."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Distributed Monitoring System:** Essential for managing distributed systems, monitoring systems track component health and trigger alerts in case of anomalies, ensuring system reliability."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Caching:** Caching systems store frequently accessed data to expedite retrieval, enhancing system performance. Data expiration policies ensure cache efficiency."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Messaging Queue:** Messaging queues store data or jobs awaiting processing, facilitating efficient task management within the system."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Pub-Sub System:** Enabling notification delivery across systems, pub-sub systems disseminate notifications to subscribers based on events. For instance, YouTube notifies subscribers of newly uploaded videos via channel subscriptions."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Rate Limiter:** Positioned before servers, rate limiters regulate incoming requests, preventing server overload. Restrictions can be applied based on IP addresses or other identifiers."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Distributed Search:** Distributed search systems enable efficient searching across vast datasets distributed across multiple nodes. They employ indexing and query distribution techniques to provide fast and scalable search capabilities. Examples include Elasticsearch and Apache Solr, which facilitate distributed indexing and querying of structured and unstructured data."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Distributed Logging:** Distributed logging systems are used to collect, aggregate, and store log data generated by various components in a distributed system. These systems ensure centralized access to logs for monitoring, troubleshooting, and analysis purposes. Popular solutions like Apache Kafka and Fluentd enable scalable, real-time log collection and processing across distributed environments."},
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "**Sharded Counters:** Sharded counters are used to maintain incrementing counters distributed across multiple nodes in a system. By partitioning the counter values across shards, these systems ensure scalability and high throughput for applications requiring fast and concurrent updates to counter values. Sharded counters are commonly employed in scenarios such as tracking user interactions, message processing, and distributed task management."}

            
           
            
    
        ],
        "card_one_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
        "image_url_card_one": "..\\static\\images\\misc\\cards.jpg",
        "card_first_url": "#",
    
        "card_two_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
        "image_url_card_two": "..\\static\\images\\misc\\cards.jpg",
    
        "card_three_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
        "image_url_card_three": "..\\static\\images\\misc\\cards.jpg",
        "first_social_media_url" : "",
        
        "second_social_media_url" : "",
        "button_link": ""
    
         }
        ,
        {
            "article_name": "federated-learning",
            "aticle_data": [
    
            {"title": "Federated Learning", "image_src": "..\\static\\images\\projects\\federated-learning\\federated-learning-flow.png", "article_para": "Federated Learning is a decentralized learning paradigm where models are trained on various devices, and their parameters are combined to create a global model. Initially introduced by Google in 2017, it allows for effective model training without transferring sensitive data from devices.", "markdown_data": ""},
    
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "## Federated Average Algorithm\nThe Federated Average Algorithm is a key component of Federated Learning, facilitating the aggregation of locally trained model parameters from multiple devices or workers into a global model. Here's a more detailed explanation of how the Federated Average Algorithm works:\n### 1. Initialization:\n- Initially, a global model with its parameters is defined. This model is typically a neural network architecture tailored for the specific task at hand (e.g., image classification, natural language processing).\n- Each participating device or worker initializes its local model with the same parameters as the global model.\n### 2. Local Model Training:\n- Each device or worker trains its local model using its own local dataset. This training process is typically performed using standard optimization techniques such as stochastic gradient descent (SGD) or its variants.\n- During training, the local model parameters are updated based on the gradients computed from the local dataset.\n### 3. Model Parameter Aggregation:\n- Once local training is complete, the updated parameters of each local model are communicated back to the central server or aggregator (often referred to as the federated server).\n- The federated server collects the parameters from all participating devices.\n### 4. Federated Averaging:\n- The federated server performs aggregation, usually through simple averaging, to compute a new set of global model parameters.\n- This aggregation process combines the parameters from all participating devices to generate a more robust and generalized global model.\n### 5. Distribution of Global Model:\n- The updated global model parameters are then distributed back to all participating devices.\n- This updated global model serves as the basis for the next round of local model training.\n### Iterative Process:\n- The entire process repeats iteratively over multiple rounds.\n- With each round, the global model tends to improve as it incorporates insights from diverse data sources and learns from different device-specific patterns.\n### Advantages of Federated Average Algorithm:\n- **Privacy Preservation**: Since raw data remains on the local devices and only model parameters are exchanged, federated learning preserves user privacy and data security.\n- **Decentralization**: Federated learning enables distributed model training across devices, reducing the need for centralized data storage and processing.\n- **Scalability**: It can scale to a large number of devices, making it suitable for applications with massive user bases.\n### Challenges and Considerations:\n- **Communication Overhead**: Communication between devices and the central server introduces latency and bandwidth constraints.\n- **Heterogeneity**: Devices may have varying computational capabilities, network conditions, and data distributions, necessitating techniques to handle heterogeneity.\n- **Model Drift**: As devices update the global model based on their local data, there is a risk of model drift, where the global model may diverge from the optimal solution due to variations in local datasets.\n- **Security Concerns**: Federated learning introduces new security risks, such as model poisoning attacks and privacy breaches, which need to be addressed through robust security measures.\nOverall, the Federated Average Algorithm forms the backbone of Federated Learning, enabling collaborative model training across distributed devices while preserving privacy and scalability."},
    
    
            {"title": "", "image_src": "", "article_para": "", "markdown_data": "### The Experiment - \nIn this program I have implement 6 workers (virtual devices) that take the MNIST data and train on 10000 data points one each. The Global model is made using the Fed-Avg Algo that is used for the aggregation of the parameters. \n ### Algo description - \n1. Since the parameters of the main model and parameters of all local models in  the nodes are randomly initialized, all these parameters will be  different from each other. For this reason, the main model sends its  parameters to the nodes before the training of local models in the nodes begins.\n2. Nodes start to train their local models over their own data by using these parameters.\n3. Each node updates its parameters while training its own model. After the  training process is completed, each node sends its parameters to the  main model.\n4. The main model takes the average of these parameters and sets them as its  new weight parameters and passes them back to the nodes for the next  iteration."},
    
            {"title": "Model Training", "image_src": "..\\static\\images\\projects\\federated-learning\\train.png", "article_para": "", "markdown_data": "### Results \n- Graph to showing accuracy of the 6 worker models in test set."},
    
            {"title": "", "image_src": "..\\static\\images\\projects\\federated-learning\\results.png", "article_para": "", "markdown_data": ""}
    
            
    
            ],
            "card_one_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
            "image_url_card_one": "..\\static\\images\\misc\\cards.jpg",
            "card_first_url": "https://www.meabhi.me",
    
            "card_two_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
            "image_url_card_two": "..\\static\\images\\misc\\cards.jpg",
            "card_two_url": "https://www.meabhi.me",
    
            "card_three_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
            "image_url_card_three": "..\\static\\images\\misc\\cards.jpg",
            "first_social_media_url" : "https://github.com/abhishekprakash256/Federated-Avearge-On-MNIST",
        
            "second_social_media_url" : "",
            "button_link":"#"
        
        
        
        },
        
        {
            "article_name": "neural-transfer",
            "aticle_data": [
           
            {"title": "Neural Style Transfer", 
            "image_src": "..\\static\\images\\projects\\neural-transfer\\image_1.png",
            "article_para": "The neural style transfer is implemented as per paper that came in 2015 title A Neural Algorithm of Artistic Style. The paper talks about combing the two images to create a new style image by using the style and feature transfer technique from both the images and tries to minimize the loss of the generated Gaussian image by using the custom loss function that can be tweaked by using the hyper-parameter alpha and beta. The implementation is done using pytorch.", 
            "markdown_data": "## Neural Style Transfer\nNeural Style Transfer (NST) is a fascinating deep learning technique that merges the artistic style of one image with the content of another, creating a visually appealing synthesis. It was introduced by Gatys et al. in their 2015 paper titled A Neural Algorithm of Artistic Style. The core idea behind NST is to utilize convolutional neural networks (CNNs) to separate and manipulate the content and style of images independently.\n### Key Components of Neural Style Transfer:\n#### Content Image:\n- The content image is the base image whose content (e.g., objects, structures) you want to retain in the final stylized image. It serves as the foundation for the overall layout and composition.\n#### Style Image:\n- The style image is the image from which you want to extract the artistic style, including textures, colors, and patterns. The goal is to imbue the content image with the stylistic features of the style image.\n#### Neural Network Features:\n- NST typically utilizes a pre-trained convolutional neural network, often VGG-19 or VGG-16, which has been trained on a large dataset for image classification tasks. These networks have learned to extract hierarchical features at different levels of abstraction.\n#### Feature Extraction:\n- Features from the content image and style image are extracted by passing them through the layers of the neural network. Different layers capture different levels of detail and abstraction. Lower layers tend to capture simple features like edges and textures, while higher layers capture more complex features like object shapes and arrangements.\n#### Gram Matrix:\n- To capture style information from the extracted features, the Gram matrix is computed. The Gram matrix represents the correlation between different feature maps in a given layer. It encodes information about texture, patterns, and colors without considering spatial information.\n#### Loss Function:\n- NST employs a loss function that balances two components: the content loss and the style loss.\n- **Content Loss**: Measures the difference between the content features of the generated image and the content image. It ensures that the generated image maintains the content of the original image.\n- **Style Loss**: Compares the Gram matrices of the style features extracted from the generated image and the style image. It ensures that the generated image captures the style of the style image."},
    
            {"title": "", 
            "image_src": "..\\static\\images\\projects\\neural-transfer\\image_3.png", 
            "article_para": "", 
            "markdown_data": "#### Optimization:\n- The goal of NST is to minimize the overall loss, which is a combination of content loss and style loss, by adjusting the pixel values of the generated image iteratively. This optimization process is typically performed using gradient descent or its variants.\n### Process of Neural Style Transfer:\n1. **Initialization**: Initialize the generated image with random pixel values or with the content image.\n2. **Feature Extraction**: Pass the content image, style image, and generated image through the neural network to extract their features at multiple layers.\n3. **Compute Loss**: Calculate the content loss between the features of the generated image and the content image, as well as the style loss between the features of the generated image and the style image.\n4. **Total Loss**: Combine the content loss and style loss using hyperparameters (alpha and beta) to get the total loss.\n5. **Optimization**: Use gradient descent to minimize the total loss by updating the pixel values of the generated image.\n6. **Iteration**: Repeat steps 2-5 iteratively until the generated image converges to a visually pleasing stylized image.\n### Applications of Neural Style Transfer:\n- **Artistic Rendering**: Create visually appealing artwork by combining the content of one image with the style of another. \n- **Image Editing**: Apply artistic styles to photographs or images to achieve a unique look and feel.\n- **Video Stylization**: Extend NST to videos, allowing for dynamic style transfer in video sequences.\n- **Texture Synthesis**: Generate novel textures by using the style of one image and applying it to random noise.\n Neural Style Transfer offers a powerful tool for artists, designers, and researchers to explore the intersection of art and artificial intelligence, enabling the creation of visually stunning images with rich artistic styles."},

            {"title": "Result", "image_src": "..\\static\\images\\projects\\neural-transfer\\transfer-image.jpg", "article_para": "", "markdown_data": ""}
    
    
    
    
    
            ],
    
            "card_one_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
            "image_url_card_one": "..\\static\\images\\misc\\cards.jpg",
            "card_first_url": "https://www.meabhi.me",
    
            "card_two_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
            "image_url_card_two": "..\\static\\images\\misc\\cards.jpg",
            "card_two_url": "https://www.meabhi.me",
    
            "card_three_text": "Some quick example text to build on the card title and make up the bulk of the card's content",
            "image_url_card_three": "..\\static\\images\\misc\\cards.jpg",
            "first_social_media_url" : "https://github.com/abhishekprakash256/Neural-Style-Transfer/tree/main",
        
            "second_social_media_url" : "",
            "button_link":"#"
        }
        
    
    ]
    
    
    
    
    
    
    
    
    
    